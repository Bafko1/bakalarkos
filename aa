# Import the required libraries
import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# Define the paths to the data folders
base_path = "LibriVoc"  # Base directory
dev_path = os.path.join(base_path, "dev-clean")  # The path to the dev-clean folder
test_path = os.path.join(base_path, "test-clean")  # The path to the test-clean folder
train_100_path = os.path.join(base_path, "train-clean-100")  # The path to the train-clean-100 folder
train_360_path = os.path.join(base_path, "train-clean-360")  # The path to the train-clean-360 folder

# Paths to the text files
dev_text_path = os.path.join(base_path, "sample2label_dev.txt")
test_text_path = os.path.join(base_path, "sample2label_test.txt")
train_text_path = os.path.join(base_path, "sample2label_train.txt")

# Define the hyperparameters
batch_size = 32 # The number of samples in each batch
num_epochs = 10 # The number of epochs to train the model
learning_rate = 0.001 # The learning rate for the optimizer
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # The device to use for training and inference


# Define the class labels for fake and real audio
fake_label = 0 # The label for fake audio
real_label = 1 # The label for real audio

# Define a custom dataset class for loading and processing the audio files
class AudioDataset(Dataset):
    """A custom dataset class for loading and processing the audio files."""

    # Define a function to load the text file containing the sample ids and labels
    def load_text_file(self, file_name):
        """Load the text file containing the sample ids and labels."""
        # Create an empty dictionary to store the sample ids and labels
        sample2label = {}
        # Open the text file in read mode
        with open(file_name, "r") as f:
            # Loop through the lines in the text file
            for line in f:
                # Split the line by whitespace
                line = line.split()
                # Get the sample id and label from the line
                sample_id = line[0]
                label = int(line[1])
                # Add the sample id and label to the dictionary
                sample2label[sample_id] = label
        # Return the dictionary
        return sample2label

    def __init__(self, root_dirs, file_name, transform=None):
        """
        Args:
            root_dirs (list of strings): A list of paths to the root directories containing the subdirectories with audio files.
            file_name (string): The path to the text file containing the sample ids and labels.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.transform = transform
        self.audio_files = []
        self.sampling_rate = 16000

        # Load the text file using the load_text_file function
        sample2label = self.load_text_file(file_name)
        print(f"Loaded {len(sample2label)} samples from {file_name}")

        # Iterate over each root directory
        for root_dir in root_dirs:
            # Loop through the subdirectories in the root directory
            for sub_dir in os.listdir(root_dir):
                # Get the full path of the subdirectory
                sub_dir_path = os.path.join(root_dir, sub_dir)
                # Loop through the inner subdirectories
                for inner_sub_dir in os.listdir(sub_dir_path):
                    # Get the full path of the inner subdirectory
                    inner_sub_dir_path = os.path.join(sub_dir_path, inner_sub_dir)
                    # Loop through the audio files in the inner subdirectory
                    for audio_file in os.listdir(inner_sub_dir_path):
                        # Get the full path of the audio file
                        audio_file_path = os.path.join(inner_sub_dir_path, audio_file)
                        # Get the sample id of the audio file by removing the extension
                        sample_id = os.path.splitext(audio_file)[0]
                        # Get the label from the dictionary
                        label = sample2label.get(sample_id, None)
                        if label is not None:
                            # Append the path and label of the audio file to the list
                            self.audio_files.append((audio_file_path, label))
                            #print(f"Added audio file: {audio_file_path} with label: {label}")



    def __len__(self):
        """Return the length of the dataset."""
        return len(self.audio_files)

    def __getitem__(self, idx):
        """Return a sample from the dataset."""
        if torch.is_tensor(idx):
            idx = idx.tolist()
        # Get the path and label of the audio file at the given index
        audio_file_path, label = self.audio_files[idx]
        # Load the audio file using torchaudio
        waveform, sample_rate = torchaudio.load(audio_file_path)
        # Resample the waveform to match the sampling rate
        resampler = torchaudio.transforms.Resample(sample_rate, self.sampling_rate)
        waveform = resampler(waveform)
        # Convert the waveform to a numpy array
        waveform = waveform.numpy()
        # Apply the transform on the waveform if specified
        if self.transform:
            waveform = self.transform(waveform)
        # Convert the waveform and label to tensors
        waveform = torch.from_numpy(waveform)
        label = torch.tensor(label)
        # Return a dictionary containing the waveform and label as a sample
        sample = {"waveform": waveform, "label": label}
        return sample

# Define a function to plot a waveform using matplotlib.pyplot 
def plot_waveform(waveform, title="Waveform"):
    """Plot a waveform using matplotlib.pyplot."""
    plt.figure()
    plt.title(title)
    plt.plot(waveform.T)
    plt.show()

# Define a function to compute and print various metrics given the true and predicted labels
def print_metrics(true_labels, pred_labels):
    """Compute and print various metrics given the true and predicted labels."""
    # Compute the accuracy, precision, recall and F1-score
    accuracy = accuracy_score(true_labels, pred_labels)
    precision = precision_score(true_labels, pred_labels)
    recall = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)
    # Print the metrics
    print("Accuracy: {:.4f}".format(accuracy))
    print("Precision: {:.4f}".format(precision))
    print("Recall: {:.4f}".format(recall))
    print("F1-score: {:.4f}".format(f1))



# Define a custom collate function
def collate_fn(batch):
    """Collate a batch of audio samples."""
    # Get the waveforms and labels from the batch
    waveforms = [sample["waveform"] for sample in batch]
    labels = [sample["label"] for sample in batch]
    # Get the maximum length of the waveforms
    max_len = max(waveform.shape[1] for waveform in waveforms)
    # Create an empty tensor of shape (batch_size, 1, max_len)
    padded_waveforms = torch.zeros(len(waveforms), 1, max_len)
    # Loop through the waveforms and pad or crop them to match the max_len
    for i, waveform in enumerate(waveforms):
        if waveform.shape[1] > max_len:
            # Crop the waveform
            padded_waveforms[i, :, :max_len] = waveform[:, :max_len]
        else:
            # Pad the waveform with zeros
            padded_waveforms[i, :, :waveform.shape[1]] = waveform
    # Stack the labels into a tensor
    labels = torch.stack(labels)
    # Return the padded waveforms and labels as a batch
    return padded_waveforms, labels


# Define the model class for fake audio detection using temporal convolution
class TemporalConvNet(torch.nn.Module):
    """A model class for fake audio detection using temporal convolution."""

    def __init__(self):
        """Initialize the model."""
        super(TemporalConvNet, self).__init__()
        # Define the convolutional layers
        self.conv1 = torch.nn.Conv1d(in_channels=1, out_channels=16, kernel_size=64, stride=2, padding=32, bias=False)
        self.conv2 = torch.nn.Conv1d(in_channels=16, out_channels=32, kernel_size=32, stride=2, padding=0, bias=False)
        self.conv3 = torch.nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=2, padding=0, bias=False)
        self.conv4 = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=8, stride=2, padding=0, bias=False)
        self.conv5 = torch.nn.Conv1d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=0, bias=False)
        self.conv6 = torch.nn.Conv1d(in_channels=256, out_channels=512, kernel_size=2, stride=2, padding=0, bias=False)
        # Define the batch normalization layers
        self.bn1 = torch.nn.BatchNorm1d(num_features=16)
        self.bn2 = torch.nn.BatchNorm1d(num_features=32)
        self.bn3 = torch.nn.BatchNorm1d(num_features=64)
        self.bn4 = torch.nn.BatchNorm1d(num_features=128)
        self.bn5 = torch.nn.BatchNorm1d(num_features=256)
        self.bn6 = torch.nn.BatchNorm1d(num_features=512)
        # Define the activation function
        self.relu = torch.nn.ReLU()
        # Define the dropout layer
        self.dropout = torch.nn.Dropout(p=0.5)
        # Define the linear layer
        self.linear = torch.nn.Linear(in_features=512*16*8*4*2*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*1*, out_features=2) # The input size is calculated by multiplying the output sizes of each convolutional layer
        # Define the softmax layer
        self.softmax = torch.nn.Softmax(dim=-1)

    def forward(self, x):
        """Perform the forward pass of the model."""
        # Apply the first convolutional layer
        x = self.conv1(x)
        # Apply the batch normalization
        x = self.bn1(x)
        # Apply the activation function
        x = self.relu(x)
        # Apply the dropout
        x = self.dropout(x)
        # Apply the second convolutional layer
        x = self.conv2(x)
        # Apply the batch normalization
        x = self.bn2(x)
        # Apply the activation function
        x = self.relu(x)
        # Apply the dropout
        x = self.dropout(x)
        # Apply the third convolutional layer
        x = self.conv3(x)
        # Apply the batch normalization
        x = self.bn3(x)
        # Apply the activation function
        x = self.relu(x)
        # Apply the dropout
        x = self.dropout(x)
        # Apply the fourth convolutional layer
        x = self.conv4(x)
        # Apply the batch normalization
        x = self.bn4(x)
        # Apply the activation function
        x = self.relu(x)
        # Apply the dropout
        x = self.dropout(x)
        # Apply the fifth convolutional layer
        x = self.conv5(x)
        # Apply the batch normalization
        x = self.bn5(x)
        # Apply the activation function
        x = self.relu(x)
        # Apply the dropout
        x = self.dropout(x)
         # Apply the sixth convolutional layer
         x = self.conv6(x)
         # Apply the batch normalization
         x = self.bn6(x)
         # Apply the activation function
         x = self.relu(x)
         # Apply the dropout
         x = self.dropout(x)
         # Flatten the output of the last convolutional layer
         x = torch.flatten(x, 1) 
         # Apply the linear layer
         x = self.linear(x) 
         # Apply the softmax layer
         x = self.softmax(x) 
         # Return the output of the model
         return x





if __name__ == '__main__':
    # Create the dev, test and train datasets using the custom dataset class
    dev_dataset = AudioDataset([dev_path], dev_text_path)
    test_dataset = AudioDataset([test_path], test_text_path)
    train_dataset = AudioDataset([train_100_path, train_360_path], train_text_path)

    # Create the dev, test and train dataloaders using the datasets
    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    # Create the train dataloader with the collate function
    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)
    # Create an instance of the model
    model = TemporalConvNet().to(device)

    # Define the loss function and optimizer
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # Train the model
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for batch in train_dataloader:
            # Get the inputs and labels from the batch
            inputs, labels = batch[0].to(device), batch[1].to(device)
            # Zero the parameter gradients
            optimizer.zero_grad()
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            # Backward pass and optimization
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        # Print the average training loss for the epoch
        print("Epoch [{}/{}], Train Loss: {:.4f}".format(epoch+1, num_epochs, train_loss/len(train_dataloader)))

    # Validate the model
    model.eval()
    val_loss = 0.0
    val_true_labels = []
    val_pred_labels = []
    with torch.no_grad():
        for batch in val_dataloader:
            # Get the inputs and labels from the batch
            inputs, labels = batch["waveform"].to(device), batch["label"].to(device)
            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            # Get the predicted labels
            _, preds = torch.max(outputs, 1)
            val_true_labels.extend(labels.cpu().numpy())
            val_pred_labels.extend(preds.cpu().numpy())
    # Print the average validation loss and metrics
    print("Validation Loss: {:.4f}".format(val_loss/len(val_dataloader)))
    print_metrics(val_true_labels, val_pred_labels)

    # Test the model
    test_true_labels = []
    test_pred_labels = []
    with torch.no_grad():
        for batch in test_dataloader:
            # Get the inputs and labels from the batch
            inputs, labels = batch["waveform"].to(device), batch["label"].to(device)
            # Forward pass
            outputs = model(inputs)
            # Get the predicted labels
            _, preds = torch.max(outputs, 1)
            test_true_labels.extend(labels.cpu().numpy())
            test_pred_labels.extend(preds.cpu().numpy())
    # Print the test metrics
    print_metrics(test_true_labels, test_pred_labels)
